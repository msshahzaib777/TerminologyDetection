{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0447e63-fd42-4cf2-be03-8aa19b2b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8561f3e9-b188-4166-8e0d-48859b58a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \".\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf5fab5-0811-4eed-85f7-22735a3dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(search_path):\n",
    "    result = []\n",
    "    # Wlaking top-down from the root\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:       \n",
    "                result.append(os.path.join(root, file))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0db0a9-c5fa-487b-93b8-e9bf028b9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = find_files(dataPath + \"\\\\train\")\n",
    "testFiles = find_files(dataPath + \"\\\\test\")\n",
    "devFiles = find_files(dataPath + \"\\\\dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5a3677-8b19-4e7a-9644-d170a856d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(paths):\n",
    "    data = pd.DataFrame(columns=[\"tokens\", \"label\"])\n",
    "    for i in paths:\n",
    "        try:\n",
    "            doc = pd.read_csv(i, sep=\"\\t\", names=[\"tokens\", \"label\"], header=None)\n",
    "            doc['file'] = i[12:-5]\n",
    "            data = pd.concat([data, doc], ignore_index=True)\n",
    "        except Exception as e: \n",
    "            print(i, e)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac53450-2fc6-460f-8f4d-f499415126a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size before Cleaning\n",
      "Training Size:\t (21549, 3)\n",
      "Test Size:\t (2781, 3)\n",
      "Dev Size:\t (2414, 3)\n"
     ]
    }
   ],
   "source": [
    "train = loadData(trainFiles)\n",
    "test  = loadData(testFiles)\n",
    "dev = loadData(devFiles)\n",
    "print(\"Training Size before Cleaning\")\n",
    "print(\"Training Size:\\t\", train.shape)\n",
    "print(\"Test Size:\\t\", test.shape)\n",
    "print(\"Dev Size:\\t\", dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22b72be-72c5-439b-b3dd-82894d6eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripSpaces(x):\n",
    "    x = unidecode(x)\n",
    "    specialchar = \"!@#$%^&*()[]{};:,./<>?\\|`-~=_+\\t\\n\"\n",
    "    for tag in specialchar:\n",
    "        x = x.replace(tag, '')\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def cleaning(dataset, verbose=True):\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isna()].index, inplace=True)\n",
    "    if verbose: \n",
    "        print(\"Size after Dropping Null Tokens\",dataset.shape)\n",
    "        print(\"Tokens Without labels:\")\n",
    "    for indexWithNullLabel in dataset[dataset[\"label\"].isna()].index:\n",
    "        token = dataset[\"tokens\"][indexWithNullLabel]\n",
    "        #split with ' ' doesnt consider multiple spaces as one\n",
    "        tokenslist = token.split()\n",
    "        dataset[\"tokens\"][indexWithNullLabel] = tokenslist[0]\n",
    "\n",
    "        if (len(tokenslist) > 1):\n",
    "            dataset[\"label\"][indexWithNullLabel] = tokenslist[1]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(dataset.loc[indexWithNullLabel, :])\n",
    "            #Manual Correction for 5467 and 5858 (very & research)\n",
    "            dataset[\"label\"][indexWithNullLabel] = 'O'\n",
    "            if verbose:\n",
    "                print(\"Manual Corrected:\", dataset[\"tokens\"][indexWithNullLabel])\n",
    "    dataset = dataset.applymap(stripSpaces)\n",
    "    #label to handel 0, i*, b*, o*, 0*\n",
    "    dataset[dataset[\"label\"] == 'ii'] = 'i'\n",
    "    dataset[dataset[\"label\"] == '0'] = 'o'\n",
    "    if verbose:\n",
    "        print(\"Removing special characters\")\n",
    "    specialCharTokens = dataset[~(dataset[\"tokens\"].str.isalnum())][\"tokens\"].unique()\n",
    "    #for sprecialChar with label B, moving label to next row and droping rows  \n",
    "    specialCharWithB = dataset[dataset[\"tokens\"].isin(specialCharTokens) & (dataset[\"label\"] == 'b')].index\n",
    "    for i in specialCharWithB:\n",
    "        dataset.loc[i+1, \"label\"] = 'b'\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(specialCharTokens) & ((dataset[\"label\"] == 'o') | (dataset[\"label\"] == 'b') )].index, inplace=True)\n",
    "    #Drop i where there is i and b before it\n",
    "    toDrop = []\n",
    "    for i in dataset[dataset[\"tokens\"].isin(specialCharTokens)].index:\n",
    "        if(dataset[\"label\"][i-1] == 'b' or dataset[\"label\"][i-1] == 'i' ):\n",
    "            toDrop.append(i)\n",
    "        else:\n",
    "            dataset[\"label\"][i] = 'b'\n",
    "    dataset.drop(toDrop, axis=0, inplace=True)\n",
    "    if verbose:\n",
    "        print(dataset.value_counts()[:30])\n",
    "        print(\"Removing Stopwords based on above listed most frequent words\")\n",
    "    stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]\n",
    "    if verbose:\n",
    "        print(\"Label order correction:\")\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    temp = dataset.copy()\n",
    "    temp[\"before\"] = temp[\"label\"].shift(1)\n",
    "    temp[\"after\"] = temp[\"label\"].shift(-1)\n",
    "    for i in temp[(temp[\"label\"] == 'i') & (temp[\"before\"] == 'o') ].index:\n",
    "            # oio or oii\n",
    "            print(temp.loc[i-1, \"tokens\"]+\"(\"+temp.loc[i-1, \"label\"]+\")\\t\\t\", temp.loc[i, \"tokens\"]+\"(\"+temp.loc[i, \"label\"]+\")\\t\\t\", temp.loc[i+1, \"tokens\"]+\"(\"+temp.loc[i+1, \"label\"]+\")\")\n",
    "            if(temp.loc[i+1, \"label\"] == 'o' or temp.loc[i+1, \"label\"] == 'i'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "            # oib\n",
    "            if(temp.loc[i+1, \"label\"] == 'b'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "                dataset.loc[i+1, \"label\"] = 'i'\n",
    "    del temp\n",
    "    temp = dataset.copy()\n",
    "    temp[(temp[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3021768c-85f8-4499-8662-66140c10d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cleaning(train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e778aebb-fae5-46b8-a1f7-f6560a1cbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b82ad-bba3-4614-9941-7ba24bb44beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of and the is often used in between \n",
    "# And seprates the Keywords may be good to not remove index [:]\n",
    "# Arabic NER only NER is Keyword\n",
    "\n",
    "#change to o and next one to b\n",
    "# 887, 12087, 13296 \n",
    "\n",
    "#state of the art\n",
    "# 2077, 2145, 2364, 5457, 5683, 6826, 6879,9720, 11570 12038 12454 12869 14087 14143 14204 14952 \n",
    "\n",
    "#remove all is, bs and is\n",
    "#  11819 11832 12006 12125 12597 \n",
    "\n",
    "#correct it\n",
    "# 11855  11878 12521 12578 12664 12665 12682 13072 13082 13166 13224 13301 15732 18273 \n",
    "\n",
    "#check later\n",
    "# 7943 11620 11695 12024 13990 \n",
    "\n",
    "#name of a person\n",
    "# 14237 14288 \n",
    "\n",
    "for i in temp[(temp[\"tokens\"].isin(temp[(temp[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index)) & (temp[\"label\"] != 'o')].index[:]:\n",
    "    print(temp.loc[i, 'file'] +\"(\",i,\"):\"+\n",
    "          temp.loc[i-4, 'tokens']+\"(\" + temp.loc[i-4, 'label'] +\") \"+ temp.loc[i-3, 'tokens']+\"(\" + temp.loc[i-3, 'label'] +\") \"+\n",
    "          temp.loc[i-2, 'tokens']+\"(\" + temp.loc[i-2, 'label'] +\") \"+ temp.loc[i-1, 'tokens']+\"(\" + temp.loc[i-1, 'label'] +\") \"+\n",
    "          temp.loc[i, 'tokens']+\"(\" + temp.loc[i, 'label'] +\") \"+ temp.loc[i+1, 'tokens']+\"(\" + temp.loc[i+1, 'label'] +\") \"+\n",
    "          temp.loc[i+2, 'tokens']+\"(\" + temp.loc[i+2, 'label'] +\") \"+ temp.loc[i+3, 'tokens']+\"(\" + temp.loc[i+3, 'label'] +\") \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec783122-9ef5-4a79-8dd9-92191a00271f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDMC",
   "language": "python",
   "name": "idmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
