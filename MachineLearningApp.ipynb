{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0447e63-fd42-4cf2-be03-8aa19b2b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8561f3e9-b188-4166-8e0d-48859b58a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \".\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf5fab5-0811-4eed-85f7-22735a3dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(search_path):\n",
    "    result = []\n",
    "    # Wlaking top-down from the root\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:       \n",
    "                result.append(os.path.join(root, file))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0db0a9-c5fa-487b-93b8-e9bf028b9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = find_files(dataPath + \"\\\\train\")\n",
    "testFiles = find_files(dataPath + \"\\\\test\")\n",
    "devFiles = find_files(dataPath + \"\\\\dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5a3677-8b19-4e7a-9644-d170a856d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(paths):\n",
    "    data = pd.DataFrame(columns=[\"tokens\", \"label\"])\n",
    "    for i in paths:\n",
    "        try:\n",
    "            doc = pd.read_csv(i, sep=\"\\t\", names=[\"tokens\", \"label\"], header=None)\n",
    "            doc['file'] = i[12:-5]\n",
    "            data = pd.concat([data, doc], ignore_index=True)\n",
    "        except Exception as e: \n",
    "            print(i, e)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac53450-2fc6-460f-8f4d-f499415126a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size before Cleaning\n",
      "Training Size:\t (21549, 3)\n",
      "Test Size:\t (2781, 3)\n",
      "Dev Size:\t (2414, 3)\n"
     ]
    }
   ],
   "source": [
    "train = loadData(trainFiles)\n",
    "test  = loadData(testFiles)\n",
    "dev = loadData(devFiles)\n",
    "print(\"Training Size before Cleaning\")\n",
    "print(\"Training Size:\\t\", train.shape)\n",
    "print(\"Test Size:\\t\", test.shape)\n",
    "print(\"Dev Size:\\t\", dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22b72be-72c5-439b-b3dd-82894d6eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripSpaces(x):\n",
    "    x = unidecode(x)\n",
    "    specialchar = \"!@#$%^&*()[]{};:,./<>?\\|`-~=_+\\t\\n\"\n",
    "    for tag in specialchar:\n",
    "        x = x.replace(tag, '')\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def cleaning(dataset):\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isna()].index, inplace=True)\n",
    "    print(\"Size after Dropping Null Tokens\",dataset.shape)\n",
    "    print(\"Tokens Without labels:\")\n",
    "    for indexWithNullLabel in dataset[dataset[\"label\"].isna()].index:\n",
    "        token = dataset[\"tokens\"][indexWithNullLabel]\n",
    "        #split with ' ' doesnt consider multiple spaces as one\n",
    "        tokenslist = token.split()\n",
    "        dataset[\"tokens\"][indexWithNullLabel] = tokenslist[0]\n",
    "\n",
    "        if (len(tokenslist) > 1):\n",
    "            dataset[\"label\"][indexWithNullLabel] = tokenslist[1]\n",
    "        else:\n",
    "            print(dataset.loc[indexWithNullLabel, :])\n",
    "            #Manual Correction for 5467 and 5858 (very & research)\n",
    "            dataset[\"label\"][indexWithNullLabel] = 'O'\n",
    "            print(\"Manual Corrected:\", dataset[\"tokens\"][indexWithNullLabel])\n",
    "    dataset = dataset.applymap(stripSpaces)\n",
    "    #label to handel 0, i*, b*, o*, 0*\n",
    "    dataset[dataset[\"label\"] == 'ii'] = 'i'\n",
    "    dataset[dataset[\"label\"] == '0'] = 'o'\n",
    "    print(\"Removing special characters\")\n",
    "    specialCharTokens = dataset[~(dataset[\"tokens\"].str.isalnum())][\"tokens\"].unique()\n",
    "    #for sprecialChar with label B, moving label to next row and droping rows  \n",
    "    specialCharWithB = dataset[dataset[\"tokens\"].isin(specialCharTokens) & (dataset[\"label\"] == 'b')].index\n",
    "    for i in specialCharWithB:\n",
    "        dataset.loc[i+1, \"label\"] = 'b'\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(specialCharTokens) & ((dataset[\"label\"] == 'o') | (dataset[\"label\"] == 'b') )].index, inplace=True)\n",
    "    #Drop i where there is i and b before it\n",
    "    toDrop = []\n",
    "    for i in dataset[dataset[\"tokens\"].isin(specialCharTokens)].index:\n",
    "        if(dataset[\"label\"][i-1] == 'b' or dataset[\"label\"][i-1] == 'i' ):\n",
    "            toDrop.append(i)\n",
    "        else:\n",
    "            dataset[\"label\"][i] = 'b'\n",
    "    dataset.drop(toDrop, axis=0, inplace=True)\n",
    "    print(dataset.value_counts()[:30])\n",
    "    print(\"Removing Stopwords based on above listed most frequent words\")\n",
    "    stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]\n",
    "    print(\"Label order correction:\")\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    temp = dataset.copy()\n",
    "    temp[\"before\"] = temp[\"label\"].shift(1)\n",
    "    temp[\"after\"] = temp[\"label\"].shift(-1)\n",
    "    for i in temp[(temp[\"label\"] == 'i') & (temp[\"before\"] == 'o') ].index:\n",
    "            # oio or oii\n",
    "            print(temp.loc[i-1, \"tokens\"]+\"(\"+temp.loc[i-1, \"label\"]+\")\\t\\t\", temp.loc[i, \"tokens\"]+\"(\"+temp.loc[i, \"label\"]+\")\\t\\t\", temp.loc[i+1, \"tokens\"]+\"(\"+temp.loc[i+1, \"label\"]+\")\")\n",
    "            if(temp.loc[i+1, \"label\"] == 'o' or temp.loc[i+1, \"label\"] == 'i'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "            # oib\n",
    "            if(temp.loc[i+1, \"label\"] == 'b'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "                dataset.loc[i+1, \"label\"] = 'i'\n",
    "    del temp\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3021768c-85f8-4499-8662-66140c10d6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Dropping Null Tokens (21487, 3)\n",
      "Tokens Without labels:\n",
      "tokens                    .\n",
      "label                   NaN\n",
      "file      \\2009-35-1-29-46.\n",
      "Name: 107, dtype: object\n",
      "Manual Corrected: .\n",
      "tokens                    very\n",
      "label                      NaN\n",
      "file      \\2013-39-3-511–554 .\n",
      "Name: 7108, dtype: object\n",
      "Manual Corrected: very\n",
      "tokens               research\n",
      "label                     NaN\n",
      "file      \\2013-39-4-847–884.\n",
      "Name: 7557, dtype: object\n",
      "Manual Corrected: research\n",
      "Removing special characters\n",
      "tokens  label  file         \n",
      "the     o      2015411120       23\n",
      "               2016421121161    22\n",
      "               2011374657688    21\n",
      "               2016422245275    21\n",
      "               20154112140      20\n",
      "of      o      2016421121161    20\n",
      "the     o      2009351328       20\n",
      "               2012383527574    20\n",
      "               2013391121160    20\n",
      "of      o      2011374657688    19\n",
      "the     o      2016424661701    19\n",
      "               2020461152       19\n",
      "               2015412185214    18\n",
      "               2011374753809    18\n",
      "               202147169116     18\n",
      "and     o      2013393555590    18\n",
      "the     o      2019451163197    17\n",
      "of      o      202147169116     16\n",
      "the     o      2009353399433    16\n",
      "               201036171109     16\n",
      "               2017434683722    15\n",
      "               2021471117140    15\n",
      "a       o      2013391121160    15\n",
      "of      o      2015411120       15\n",
      "the     o      2019451137161    15\n",
      "               2017433521565    15\n",
      "               2010362159201    15\n",
      "               2013392267300    14\n",
      "that    o      2011374753809    14\n",
      "the     o      2013392229266    13\n",
      "dtype: int64\n",
      "Removing Stopwords based on above listed most frequent words\n",
      "Label order correction:\n",
      "successful(o)\t\t techniques(i)\t\t and(o)\n",
      "2003(o)\t\t parsing(i)\t\t model(i)\n",
      "authorship(o)\t\t attribution(i)\t\t of(o)\n",
      "on(o)\t\t error(i)\t\t correction(i)\n",
      "of(o)\t\t compositionality(i)\t\t with(o)\n",
      "the(o)\t\t models(i)\t\t and(o)\n",
      "a(o)\t\t complex(i)\t\t graph(b)\n",
      "into(o)\t\t simple(i)\t\t subgraphs(b)\n"
     ]
    }
   ],
   "source": [
    "train = cleaning(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e778aebb-fae5-46b8-a1f7-f6560a1cbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bc2d2-9dc4-4f9e-ac76-7f6e6f0f2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train[\"tokens\"].isin(stopwords)) & (train[\"label\"] != 'o')].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bcf39-d980-4abb-aff2-d8d55f46489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tokens.extend(train[\"tokens\"].tolist())\n",
    "tokenset = set(tokens)\n",
    "print(\"Unique Tokens in Train and dev set: \",len(tokenset))\n",
    "tokenset\n",
    "#[x for x in tokenset if len(x)<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1714a8dd-15f3-42f6-82c8-88dc33308e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords with o to investigate stopwords with b and i\n",
    "temp = train.copy()\n",
    "temp.drop(temp[(temp[\"tokens\"].isin(stopwords)) & (temp[\"label\"] == 'o')].index, axis=0, inplace=True)\n",
    "temp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cebf82d8-d446-4646-ab2a-141884f4c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens  label  file         \n",
       "to      i      2019452267292    5\n",
       "the     i      2017434683722    4\n",
       "               2013394847884    2\n",
       "               2018443403446    2\n",
       "and     i      2018443447482    2\n",
       "to      i      20093512946      2\n",
       "and     i      2017431181200    2\n",
       "               2016424661701    2\n",
       "to      i      2021472309332    2\n",
       "               2021472445476    2\n",
       "a       i      2017434781835    2\n",
       "the     i      2017434781835    2\n",
       "               2017431125179    1\n",
       "               2016421121161    1\n",
       "               2017431181200    1\n",
       "               201743171123     1\n",
       "               2017433521565    1\n",
       "a       i      2017434683722    1\n",
       "the     i      2017434723760    1\n",
       "               2018442349374    1\n",
       "               20133912355      1\n",
       "their   i      2017431181200    1\n",
       "to      i      2014402349401    1\n",
       "               2020461152       1\n",
       "with    i      2017433465520    1\n",
       "the     i      2019451163197    1\n",
       "               2010363303339    1\n",
       "               2013391195227    1\n",
       "are     i      2017434781835    1\n",
       "all     b      2017433465520    1\n",
       "        i      2017434781835    1\n",
       "an      i      2017431125179    1\n",
       "and     i      201036171109     1\n",
       "               201440185120     1\n",
       "               2016423391419    1\n",
       "               2018442329348    1\n",
       "               2020461152       1\n",
       "for     i      2019452229265    1\n",
       "the     i      2010362247277    1\n",
       "for     i      2021472309332    1\n",
       "from    b      201743171123     1\n",
       "in      b      2009353313343    1\n",
       "        i      201743171123     1\n",
       "               2017434683722    1\n",
       "on      i      2014401171202    1\n",
       "               201743171123     1\n",
       "the     i      2010362203227    1\n",
       "with    i      2017434683722    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[(temp[\"tokens\"].isin(stopwords))].value_counts()\n",
    "# temp[(temp[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3b82ad-bba3-4614-9941-7ba24bb44beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20093512946:both(o) text(b) to(i) text(i) concept(b) to(i) \n",
      "20093512946:text(i) concept(b) to(i) text(i) generation(i) systems(i) \n",
      "2009353313343:of(o) example(o) in(b) domain(i) training(i) sentences(i) \n",
      "201036171109:of(o) extrinsic(b) and(i) intrinsic(i) measures(i) outcome(o) \n",
      "2010362203227:state(b) of(i) the(i) art(i) classification(b) method(i) \n",
      "2010362247277:state(b) of(i) the(i) art(i) syntax(b) mt(i) \n",
      "2010363303339:state(b) of(i) the(i) art(i) alignment(b) quality(i) \n",
      "2013391195227:state(b) of(i) the(i) art(i) system(o) based(o) \n",
      "20133912355:state(b) of(i) the(i) art(i) data(b) driven(i) \n",
      "2013394847884:state(b) of(i) the(i) art(i) such(o) linking(b) \n",
      "2013394847884:state(b) of(i) the(i) art(i) level(o) participated(o) \n",
      "2014401171202:called(o) arabic(b) on(i) line(i) commentary(i) data(b) \n",
      "201440185120:especially(o) sparse(b) and(i) polysemous(i) words(i) frame(b) \n",
      "2014402349401:languages(b) tree(b) to(i) tree(i) translation(b) evidence(o) \n",
      "2016421121161:state(b) of(i) the(i) art(i) smt(b) model(i) \n",
      "2016423391419:both(o) paradigmatic(b) and(i) syntagmatic(i) relations(i) linguistically(b) \n",
      "2016424661701:of(o) subject(b) and(i) object(i) relative(i) clauses(i) \n",
      "2016424661701:across(o) subject(b) and(i) object(i) relative(i) clauses(i) \n",
      "2017431125179:state(b) of(i) the(i) art(i) several(o) ways(o) \n",
      "2017431125179:by(o) adapting(b) an(i) argumentation(i) model(i) tested(o) \n",
      "2017431181200:blogs(i) characterize(b) the(i) topic(i) of(o) message(o) \n",
      "2017431181200:been(o) analyze(b) their(i) context(i) just(o) pointed(o) \n",
      "2017431181200:hashtags(b) multiple(b) and(i) variable(i) meanings(i) article(o) \n",
      "2017431181200:hashtags(b) similar(b) and(i) synchronous(i) usage(i) patterns(i) \n",
      "201743171123:computational(b) work(i) on(i) metaphor(i) traditionally(o) evolved(o) \n",
      "201743171123:metaphorical(b) association(i) from(b) text(i) order(o) investigate(o) \n",
      "201743171123:them(o) data(b) in(i) three(i) languages(i) different(b) \n",
      "201743171123:state(b) of(i) the(i) i(i) art(i) results(o) \n",
      "2017433465520:similarity(b) measures(i) all(b) subtree(i) kernels(i) compare(o) \n",
      "2017433465520:regarding(o) correlation(b) with(i) human(i) judgments(i) both(o) \n",
      "2017433521565:state(b) of(i) the(i) art(i) performance(i) several(o) \n",
      "2017434683722:methods(b) analyzing(b) the(i) activation(i) patterns(i) of(o) \n",
      "2017434683722:trained(b) predicting(b) the(i) representations(i) of(o) visual(o) \n",
      "2017434683722:trained(b) predict(b) the(i) next(i) word(i) same(o) \n",
      "2017434683722:function(i) position(b) in(i) the(i) sequential(i) structure(i) \n",
      "2017434683722:position(b) in(i) the(i) sequential(i) structure(i) of(o) \n",
      "2017434683722:sensitive(o) words(b) with(i) a(i) syntactic(i) function(i) \n",
      "2017434683722:words(b) with(i) a(i) syntactic(i) function(i) further(o) \n",
      "2017434723760:state(b) of(i) the(i) art(i) le(b) distributional(b) \n",
      "2017434781835:first(o) devise(b) a(i) hierarchical(i) alignment(i) scheme(i) \n",
      "2017434781835:parse(b) trees(i) are(i) aligned(i) way(o) eliminates(o) \n",
      "2017434781835:distribution(b) of(i) the(i) translation(i) divergences(i) shows(o) \n",
      "2017434781835:enough(o) capture(b) the(i) translation(i) divergences(i) point(o) \n",
      "2017434781835:thus(o) building(b) a(i) semantic(i) representation(i) captures(b) \n",
      "2017434781835:representation(i) captures(b) all(i) possible(i) translation(i) divergences(i) \n",
      "2018442329348:testable(i) left(b) and(i) right(i) context(i) k(i) \n",
      "2018442349374:state(b) of(i) the(i) art(i) unsupervised(b) morphological(i) \n",
      "2018443403446:state(b) of(i) the(i) art(i) however(o) systematic(b) \n",
      "2018443403446:state(b) of(i) the(i) art(i) results(o) several(o) \n",
      "2018443447482:vijay(i) shanker(i) and(i) weir(i) 1994(o) main(o) \n",
      "2018443447482:vijay(i) shanker(i) and(i) weir(i) 1994(o) apart(o) \n",
      "2019451163197:state(b) of(i) the(i) art(i) matching(o) methods(o) \n",
      "2019452229265:new(o) models(b) for(i) automatic(i) annotation(i) neural(b) \n",
      "2019452267292:based(o) sequence(b) to(i) sequence(i) learning(i) encodes(b) \n",
      "2019452267292:novel(o) end(b) to(i) end(i) syntactic(i) nmt(i) \n",
      "2019452267292:call(o) tree(b) to(i) sequence(i) nmt(i) model(i) \n",
      "2019452267292:extending(o) sequence(b) to(i) sequence(i) model(i) source(b) \n",
      "2019452267292:model(o) sequence(b) to(i) sequence(i) models(i) various(o) \n",
      "2020461152:trained(b) end(b) to(i) end(i) fashion(o) without(o) \n",
      "2020461152:local(o) syntactic(b) and(i) semantic(b) dependencies(i) better(o) \n",
      "2021472309332:studies(o) text(b) to(i) sql(i) problem(o) of(o) \n",
      "2021472309332:annotation(i) network(i) for(i) sql(i) solve(o) complex(o) \n",
      "2021472309332:complex(o) text(b) to(i) sql(i) tasks(i) cross(b) \n",
      "2021472445476:schemes(o) many(b) to(i) one(i) approach(i) translates(o) \n",
      "2021472445476:while(o) one(b) to(i) many(i) approach(i) translates(o) \n"
     ]
    }
   ],
   "source": [
    "# temp[(temp[\"tokens\"].isin(stopwords))].value_counts()\n",
    "# of and the is often used in between \n",
    "#Arabic NER only NER is Keyword\n",
    "\n",
    "for i in temp[(temp[\"tokens\"].isin(temp[(temp[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index))].index:\n",
    "    print(temp.loc[i, 'file'] +\":\"+\n",
    "          temp.loc[i-2, 'tokens']+\"(\" + temp.loc[i-2, 'label'] +\") \"+ temp.loc[i-1, 'tokens']+\"(\" + temp.loc[i-1, 'label'] +\") \"+\n",
    "          temp.loc[i, 'tokens']+\"(\" + temp.loc[i, 'label'] +\") \"+ temp.loc[i+1, 'tokens']+\"(\" + temp.loc[i+1, 'label'] +\") \"+\n",
    "          temp.loc[i+2, 'tokens']+\"(\" + temp.loc[i+2, 'label'] +\") \"+ temp.loc[i+3, 'tokens']+\"(\" + temp.loc[i+3, 'label'] +\") \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec783122-9ef5-4a79-8dd9-92191a00271f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDMC",
   "language": "python",
   "name": "idmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
