{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0447e63-fd42-4cf2-be03-8aa19b2b7885",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Subset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#from torchvision import transforms, datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.10/site-packages/torch/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      4\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.10/site-packages/torch/nn/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[1;32m      4\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[1;32m      5\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.10/site-packages/torch/nn/modules/module.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.10/site-packages/torch/nn/parameter.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, pipeline, svm, linear_model, neighbors, metrics, ensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "#from torchvision import transforms, datasets\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw, wordnet as wn\n",
    "import re\n",
    "import string \n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pathlib import Path # abstract away windows/linux differences\n",
    "\n",
    "#dataPath = \".\\\\data\"\n",
    "dataPath = Path('.') / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91042bf4-67e3-4c8c-b600-a7e47c508d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac53450-2fc6-460f-8f4d-f499415126a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training Size before Cleaning\")\n",
    "# print(\"Training Size:\\t\", train.shape)\n",
    "# print(\"Test Size:\\t\", test.shape)\n",
    "# print(\"Dev Size:\\t\", dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b72be-72c5-439b-b3dd-82894d6eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"about\", \"all\", \"also\", \"among\", \"at\", \"available\", \"be\", \"because\", \"been\", \"both\", \"but\", \"by\", \"can\", \"each\", \"first\", \"has\", \"have\", \"here\", \"how\",\n",
    "             \"however\", \"into\", \"it\", \"its\", \"large\", \"learn\", \"many\", \"may\", \"more\", \"most\", \"much\", \"new\", \"not\", \"often\", \"only\", \"or\", \"other\", \"over\", \"recent\", \"related\", \"same\",\n",
    "             \"several\", \"shown\", \"some\", \"studies\", \"such\", \"than\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"those\", \"through\", \"use\", \"used\", \"we\", \"well\", \"what\",\n",
    "             \"when\", \"where\", \"which\"]\n",
    "def find_files(search_path):\n",
    "    result = []\n",
    "    # Walking top-down from the root\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:       \n",
    "                result.append(os.path.join(root, file))\n",
    "    return result\n",
    "\n",
    "def loadData(paths):\n",
    "    data = pd.DataFrame(columns=[\"tokens\", \"label\"])\n",
    "    for i in paths:\n",
    "        try:\n",
    "            doc = pd.read_csv(i, sep=\"\\t\", names=[\"tokens\", \"label\"], header=None)\n",
    "            doc['file'] = i[12:-5]\n",
    "            data = pd.concat([data, doc], ignore_index=True)\n",
    "        except Exception as e: \n",
    "            print(i, e)\n",
    "    return data\n",
    "\n",
    "def stripSpaces(x):\n",
    "    x = unidecode(x)\n",
    "    specialchar = \"!@#$%^&*()[]{};:,./<>?\\|`-~=_+\\t\\n\"\n",
    "    for tag in specialchar:\n",
    "        x = x.replace(tag, '')\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def cleaning(_dataset, lemma=True, pos=False, verbose=True):\n",
    "    dataset = _dataset.copy()\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isna()].index, inplace=True)\n",
    "    if verbose: \n",
    "        print(\"Size after Dropping Null Tokens\",dataset.shape)\n",
    "        print(\"Tokens Without labels:\")\n",
    "    for indexWithNullLabel in dataset[dataset[\"label\"].isna()].index:\n",
    "        token = dataset[\"tokens\"][indexWithNullLabel]\n",
    "        #split with ' ' doesnt consider multiple spaces as one\n",
    "        tokenslist = token.split()\n",
    "        dataset[\"tokens\"][indexWithNullLabel] = tokenslist[0]\n",
    "\n",
    "        if (len(tokenslist) > 1):\n",
    "            dataset[\"label\"][indexWithNullLabel] = tokenslist[1]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(dataset.loc[indexWithNullLabel, :])\n",
    "            #Manual Correction for 5467 and 5858 (very & research)\n",
    "            dataset[\"label\"][indexWithNullLabel] = 'O'\n",
    "            if verbose:\n",
    "                print(\"Manual Corrected:\", dataset[\"tokens\"][indexWithNullLabel])\n",
    "    dataset = dataset.applymap(stripSpaces)\n",
    "    #label to handel 0, i*, b*, o*, 0*\n",
    "    dataset[dataset[\"label\"] == 'ii'] = 'i'\n",
    "    dataset[dataset[\"label\"] == '0'] = 'o'\n",
    "    if verbose:\n",
    "        print(\"Removing special characters\")\n",
    "    specialCharTokens = dataset[~(dataset[\"tokens\"].str.isalnum())][\"tokens\"].unique()\n",
    "    #for sprecialChar with label B, moving label to next row and droping rows  \n",
    "    specialCharWithB = dataset[dataset[\"tokens\"].isin(specialCharTokens) & (dataset[\"label\"] == 'b')].index\n",
    "    for i in specialCharWithB:\n",
    "        dataset.loc[i+1, \"label\"] = 'b'\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(specialCharTokens) & ((dataset[\"label\"] == 'o') | (dataset[\"label\"] == 'b') )].index, inplace=True)\n",
    "    #Drop i where there is i and b before it\n",
    "    toDrop = []\n",
    "    for i in dataset[dataset[\"tokens\"].isin(specialCharTokens)].index:\n",
    "        if(dataset[\"label\"][i-1] == 'b' or dataset[\"label\"][i-1] == 'i' ):\n",
    "            toDrop.append(i)\n",
    "        else:\n",
    "            dataset[\"label\"][i] = 'b'\n",
    "    dataset.drop(toDrop, axis=0, inplace=True)\n",
    "    if verbose:\n",
    "        print(dataset.value_counts()[:30])\n",
    "        print(\"Removing Stopwords based on above listed most frequent words\")\n",
    "    stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]\n",
    "    if verbose:\n",
    "        print(\"Label order correction:\")\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    temp = dataset.copy()\n",
    "    temp[\"before\"] = temp[\"label\"].shift(1)\n",
    "    temp[\"after\"] = temp[\"label\"].shift(-1)\n",
    "    for i in temp[(temp[\"label\"] == 'i') & (temp[\"before\"] == 'o') ].index:\n",
    "            # oio or oii\n",
    "            if verbose:\n",
    "                print(temp.loc[i-1, \"tokens\"]+\"(\"+temp.loc[i-1, \"label\"]+\")\\t\\t\", temp.loc[i, \"tokens\"]+\"(\"+temp.loc[i, \"label\"]+\")\\t\\t\", temp.loc[i+1, \"tokens\"]+\"(\"+temp.loc[i+1, \"label\"]+\")\")\n",
    "            if(temp.loc[i+1, \"label\"] == 'o' or temp.loc[i+1, \"label\"] == 'i'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "            # oib\n",
    "            if(temp.loc[i+1, \"label\"] == 'b'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "                dataset.loc[i+1, \"label\"] = 'i'\n",
    "    del temp\n",
    "    if verbose:\n",
    "        print(dataset[(dataset[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index)\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(dataset)].index, inplace=True)\n",
    "    if pos:\n",
    "        dataset[\"POS\"] = nltk.pos_tag(train[\"tokens\"])\n",
    "    if lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        dataset[\"tokens\"] = dataset[\"tokens\"].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def featurePreparation(_dataset, ref=1):\n",
    "    dataset = _dataset.copy()\n",
    "    if  ref == 0:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"]\n",
    "    elif ref == 1:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(fill_value= \"\") + \" \" + dataset[\"tokens\"] \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\")\n",
    "    elif ref == 2:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(2, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"] + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-2, fill_value= \"\")\n",
    "    elif ref == 3:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(3, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(2, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(fill_value= \"\") + \" \" + dataset[\"tokens\"] \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-2, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(-3, fill_value= \"\")\n",
    "    dataset[\"text\"] = dataset[\"tokens\"]\n",
    "    dataset = dataset.drop([\"tokens\", \"file\"], axis=1)\n",
    "    return dataset\n",
    "\n",
    "def pre_pipeline(ref=1, pos=False,lemma=True):\n",
    "    #trainFiles = find_files(dataPath + \"\\\\train\")\n",
    "    #testFiles = find_files(dataPath + \"\\\\test\")\n",
    "    #devFiles = find_files(dataPath + \"\\\\dev\")\n",
    "    \n",
    "    trainFiles = find_files(dataPath / \"train\")\n",
    "    testFiles = find_files(dataPath / \"test\")\n",
    "    devFiles = find_files(dataPath / \"dev\")\n",
    "    \n",
    "    train = loadData(trainFiles)\n",
    "    test  = loadData(testFiles)\n",
    "    dev = loadData(devFiles)\n",
    "    train = cleaning(train, lemma=lemma, pos=pos, verbose=False)\n",
    "    test = cleaning(test, lemma=lemma, pos=pos, verbose=False)\n",
    "    dev = cleaning(dev, lemma=lemma, pos=pos, verbose=False)\n",
    "    train = featurePreparation(train, ref=ref)\n",
    "    test = featurePreparation(test, ref=ref)\n",
    "    dev = featurePreparation(dev, ref=ref)\n",
    "    return train, test, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108ab10-da77-4cb4-b92f-6cb9ec2043a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb21b7-a45a-49a2-9ca1-ef319b7b0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=3, lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc568c70-fdd2-4bcb-b051-ca5527bc334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=0, lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec898ee-d8bc-4aae-a395-e28bb408ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=0, lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d32a33-a1b0-41bc-a4fe-07786a41dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=1, lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbced54-a718-48fd-9d7f-98c0b2bb72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=3, lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d2d03-2273-40b0-91c3-a87f18890fe6",
   "metadata": {},
   "source": [
    "###    precision    recall  f1-score   support\n",
    "b       0.57      0.43      0.49       322    7-word with lemma<br>\n",
    "i       0.54      0.42      0.47       315<br>\n",
    "b       0.59      0.41      0.48       322    1-word no lemma<br>\n",
    "i       0.51      0.39      0.44       315<br>\n",
    "b       0.55      0.40      0.46       322    3-word no lemma<br>\n",
    "i       0.52      0.39      0.45       315<br>\n",
    "b       0.52      0.36      0.42       322    3-word with lemma<br>\n",
    "i       0.50      0.40      0.45       315<br>\n",
    "b       0.56      0.41      0.48       322    1-word with lemma<br>\n",
    "i       0.53      0.40      0.45       315<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9641d-6a50-44e4-82b3-47d5fe8f5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(pos=True,lemma=False, ref=0)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"i\", \"o\"])\n",
    "trainY = le.transform(train[\"label\"])\n",
    "\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "devY = le.transform(dev[\"label\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "testY = le.transform(test[\"label\"])\n",
    "\n",
    "pipe = pipeline.make_pipeline(ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "\n",
    "y_hat = pipe.predict(testX) \n",
    "print(metrics.classification_report(le.inverse_transform(testY), le.inverse_transform(y_hat)))\n",
    "metrics.confusion_matrix(le.inverse_transform(testY), le.inverse_transform(y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Terminology]",
   "language": "python",
   "name": "conda-env-Terminology-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
