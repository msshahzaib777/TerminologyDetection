{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4eb4bb2-a3d9-4fc7-90c0-3c6d178b41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/Terminology/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, pipeline, svm, linear_model, neighbors, metrics, ensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "#from torchvision import transforms, datasets\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw, wordnet as wn\n",
    "import re\n",
    "import string \n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "dataPath = \".\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980cbd51-07f6-469e-b15e-5c8e69977b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81014b72-c790-44fc-8a87-0a772471eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(search_path):\n",
    "    result = []\n",
    "    # Walking top-down from the root\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:       \n",
    "                result.append(os.path.join(root, file))\n",
    "    return result\n",
    "\n",
    "def loadData(paths):\n",
    "    data = pd.DataFrame(columns=[\"tokens\", \"label\"])\n",
    "    for i in paths:\n",
    "        try:\n",
    "            doc = pd.read_csv(i, sep=\"\\t\", names=[\"tokens\", \"label\"], header=None)\n",
    "            doc['file'] = i[12:-5]\n",
    "            data = pd.concat([data, doc], ignore_index=True)\n",
    "        except Exception as e: \n",
    "            print(i, e)\n",
    "    return data\n",
    "\n",
    "def stripSpaces(x):\n",
    "    x = unidecode(x)\n",
    "    specialchar = \"!@#$%^&*()[]{};:,./<>?\\|`-~=_+\\t\\n\"\n",
    "    for tag in specialchar:\n",
    "        x = x.replace(tag, '')\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def cleaning(_dataset, lemma=True, pos=False, verbose=True):\n",
    "    dataset = _dataset.copy()\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isna()].index, inplace=True)\n",
    "    if verbose: \n",
    "        print(\"Size after Dropping Null Tokens\",dataset.shape)\n",
    "        print(\"Tokens Without labels:\")\n",
    "    for indexWithNullLabel in dataset[dataset[\"label\"].isna()].index:\n",
    "        token = dataset[\"tokens\"][indexWithNullLabel]\n",
    "        #split with ' ' doesnt consider multiple spaces as one\n",
    "        tokenslist = token.split()\n",
    "        dataset[\"tokens\"][indexWithNullLabel] = tokenslist[0]\n",
    "\n",
    "        if (len(tokenslist) > 1):\n",
    "            dataset[\"label\"][indexWithNullLabel] = tokenslist[1]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(dataset.loc[indexWithNullLabel, :])\n",
    "            #Manual Correction for 5467 and 5858 (very & research)\n",
    "            dataset[\"label\"][indexWithNullLabel] = 'O'\n",
    "            if verbose:\n",
    "                print(\"Manual Corrected:\", dataset[\"tokens\"][indexWithNullLabel])\n",
    "    dataset = dataset.applymap(stripSpaces)\n",
    "    #label to handel 0, i*, b*, o*, 0*\n",
    "    dataset[dataset[\"label\"] == 'ii'] = 'i'\n",
    "    dataset[dataset[\"label\"] == '0'] = 'o'\n",
    "    if verbose:\n",
    "        print(\"Removing special characters\")\n",
    "    specialCharTokens = dataset[~(dataset[\"tokens\"].str.isalnum())][\"tokens\"].unique()\n",
    "    #for sprecialChar with label B, moving label to next row and droping rows  \n",
    "    specialCharWithB = dataset[dataset[\"tokens\"].isin(specialCharTokens) & (dataset[\"label\"] == 'b')].index\n",
    "    for i in specialCharWithB:\n",
    "        dataset.loc[i+1, \"label\"] = 'b'\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(specialCharTokens) & ((dataset[\"label\"] == 'o') | (dataset[\"label\"] == 'b') )].index, inplace=True)\n",
    "    #Drop i where there is i and b before it\n",
    "    toDrop = []\n",
    "    for i in dataset[dataset[\"tokens\"].isin(specialCharTokens)].index:\n",
    "        if(dataset[\"label\"][i-1] == 'b' or dataset[\"label\"][i-1] == 'i' ):\n",
    "            toDrop.append(i)\n",
    "        else:\n",
    "            dataset[\"label\"][i] = 'b'\n",
    "    dataset.drop(toDrop, axis=0, inplace=True)\n",
    "    if verbose:\n",
    "        print(dataset.value_counts()[:30])\n",
    "        print(\"Removing Stopwords based on above listed most frequent words\")\n",
    "    stopwords = [\"about\", \"all\", \"also\", \"among\", \"at\", \"available\", \"be\", \"because\", \"been\", \"both\", \"but\", \"by\", \"can\", \"each\", \"first\", \"has\", \"have\", \"here\", \"how\",\n",
    "             \"however\", \"into\", \"it\", \"its\", \"large\", \"learn\", \"many\", \"may\", \"more\", \"most\", \"much\", \"new\", \"not\", \"often\", \"only\", \"or\", \"other\", \"over\", \"recent\", \"related\", \"same\",\n",
    "             \"several\", \"shown\", \"some\", \"studies\", \"such\", \"than\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"those\", \"through\", \"use\", \"used\", \"we\", \"well\", \"what\",\n",
    "             \"when\", \"where\", \"which\"]\n",
    "    if verbose:\n",
    "        print(\"Label order correction:\")\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    temp = dataset.copy()\n",
    "    temp[\"before\"] = temp[\"label\"].shift(1)\n",
    "    temp[\"after\"] = temp[\"label\"].shift(-1)\n",
    "    for i in temp[(temp[\"label\"] == 'i') & (temp[\"before\"] == 'o') ].index:\n",
    "            # oio or oii\n",
    "            if verbose:\n",
    "                print(temp.loc[i-1, \"tokens\"]+\"(\"+temp.loc[i-1, \"label\"]+\")\\t\\t\", temp.loc[i, \"tokens\"]+\"(\"+temp.loc[i, \"label\"]+\")\\t\\t\", temp.loc[i+1, \"tokens\"]+\"(\"+temp.loc[i+1, \"label\"]+\")\")\n",
    "            if(temp.loc[i+1, \"label\"] == 'o' or temp.loc[i+1, \"label\"] == 'i'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "            # oib\n",
    "            if(temp.loc[i+1, \"label\"] == 'b'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "                dataset.loc[i+1, \"label\"] = 'i'\n",
    "    del temp\n",
    "    if verbose:\n",
    "        print(dataset[(dataset[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index)\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(stopwords)].index, inplace=True)\n",
    "    if pos:\n",
    "        dataset[\"POS\"] = nltk.pos_tag(train[\"tokens\"])\n",
    "    if lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        dataset[\"tokens\"] = dataset[\"tokens\"].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def featurePreparation(_dataset, ref=1):\n",
    "    dataset = _dataset.copy()\n",
    "    if  ref == 0:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"]\n",
    "    elif ref == 1:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(fill_value= \"\") + \" \" + dataset[\"tokens\"] \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\")\n",
    "    elif ref == 2:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(2, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"] + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-2, fill_value= \"\")\n",
    "    elif ref == 3:\n",
    "        dataset[\"text\"] = dataset[\"tokens\"].shift(3, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(2, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(fill_value= \"\") + \" \" + dataset[\"tokens\"] \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-1, fill_value= \"\") \\\n",
    "                            + \" \" + dataset[\"tokens\"].shift(-2, fill_value= \"\") + \" \" + dataset[\"tokens\"].shift(-3, fill_value= \"\")\n",
    "    dataset = dataset.drop([\"tokens\", \"file\"], axis=1)\n",
    "    return dataset\n",
    "\n",
    "def pre_pipeline(ref=1, pos=False,lemma=True):\n",
    "    trainFiles = find_files(dataPath + \"\\\\train\")\n",
    "    testFiles = find_files(dataPath + \"\\\\test\")\n",
    "    devFiles = find_files(dataPath + \"\\\\dev\")\n",
    "    train = loadData(trainFiles)\n",
    "    test  = loadData(testFiles)\n",
    "    dev = loadData(devFiles)\n",
    "    train = cleaning(train, lemma=lemma, pos=pos, verbose=False)\n",
    "    test = cleaning(test, lemma=lemma, pos=pos, verbose=False)\n",
    "    dev = cleaning(dev, lemma=lemma, pos=pos, verbose=False)\n",
    "    train = featurePreparation(train, ref=ref)\n",
    "    test = featurePreparation(test, ref=ref)\n",
    "    dev = featurePreparation(dev, ref=ref)\n",
    "    return train, test, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a059b8-d09c-4c87-ad7e-9ce463af4c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['file'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test, dev \u001b[38;5;241m=\u001b[39m \u001b[43mpre_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlemma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m      3\u001b[0m trainX \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 135\u001b[0m, in \u001b[0;36mpre_pipeline\u001b[0;34m(ref, pos, lemma)\u001b[0m\n\u001b[1;32m    133\u001b[0m test \u001b[38;5;241m=\u001b[39m cleaning(test, lemma\u001b[38;5;241m=\u001b[39mlemma, pos\u001b[38;5;241m=\u001b[39mpos, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m dev \u001b[38;5;241m=\u001b[39m cleaning(dev, lemma\u001b[38;5;241m=\u001b[39mlemma, pos\u001b[38;5;241m=\u001b[39mpos, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 135\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturePreparation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m test \u001b[38;5;241m=\u001b[39m featurePreparation(test, ref\u001b[38;5;241m=\u001b[39mref)\n\u001b[1;32m    137\u001b[0m dev \u001b[38;5;241m=\u001b[39m featurePreparation(dev, ref\u001b[38;5;241m=\u001b[39mref)\n",
      "Cell \u001b[0;32mIn[3], line 122\u001b[0m, in \u001b[0;36mfeaturePreparation\u001b[0;34m(_dataset, ref)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ref \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    118\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m3\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m2\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    119\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[1;32m    120\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    121\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/core/frame.py:5396\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5257\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5259\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5260\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5394\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5398\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/Terminology/lib/python3.11/site-packages/pandas/core/indexes/base.py:6977\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6977\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6978\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['file'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train, test, dev = pre_pipeline(ref=2,lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecafa7d-7962-4cb5-90c4-3326e491b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2173a2-ca8f-4bef-80a9-772da63749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': trainX.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': trainY.shape[1],\n",
    "    'epochs': 14,\n",
    "    'batch_size': 3000,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (devX.toarray(), devY.toarray()),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af8012-e89b-4c08-a341-9a517647a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = clf.fit(trainX.toarray(), trainY.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166178d-98e0-4ba5-9824-09fa7e0adb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \"\"\" Plot model loss and accuracy through epochs. \"\"\"    \n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "plot_model_performance(\n",
    "    train_loss=hist.history.get('loss', []),\n",
    "    train_acc=hist.history.get('accuracy', []),\n",
    "    train_val_loss=hist.history.get('val_loss', []),\n",
    "    train_val_acc=hist.history.get('val_accuracy', [])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131ec82-5292-4415-9792-e0ddae6659ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(clf.model, to_file='model.png', show_shapes=True)\n",
    "clf.model.save('./keras_mlp.h5')\n",
    "score = clf.score(testX.toarray(), testY.toarray())\n",
    "print(score)\n",
    "yhat = clf.predict(testX.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7567c79-34d9-4040-8907-d8823b8df171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09ca59-d14d-476f-862e-1fcba8d50d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=3,lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed67948-9343-4999-834b-eacd45e2abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=1,lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985fcb4-4c10-424f-b91e-896450ce9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=0,lemma=False)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70727d63-9804-4c86-b57d-6b0d21420f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=3,lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': trainX.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': trainY.shape[1],\n",
    "    'epochs': 20,\n",
    "    'batch_size': 512,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (devX.toarray(), devY.toarray()),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c41d8-e060-4f6b-ab5b-e37e08d267a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=2,lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': trainX.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': trainY.shape[1],\n",
    "    'epochs': 14,\n",
    "    'batch_size': 3000,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (devX.toarray(), devY.toarray()),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6e87a-3a7e-4bce-a68e-70094c13641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=1,lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': trainX.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': trainY.shape[1],\n",
    "    'epochs': 20,\n",
    "    'batch_size': 512,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (devX.toarray(), devY.toarray()),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e02463-0afe-4877-bd63-5c9084436dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev = pre_pipeline(ref=0,lemma=True)\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(train[\"text\"])\n",
    "testX = vectorizer.transform(test[\"text\"])\n",
    "devX = vectorizer.transform(dev[\"text\"])\n",
    "print(\"Vobac Size\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "le = preprocessing.OneHotEncoder()\n",
    "le = le.fit(train[\"label\"].values.reshape(-1,1))\n",
    "trainY = le.transform(train[\"label\"].values.reshape(-1,1))\n",
    "devY = le.transform(dev[\"label\"].values.reshape(-1,1))\n",
    "testY = le.transform(test[\"label\"].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': trainX.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': trainY.shape[1],\n",
    "    'epochs': 20,\n",
    "    'batch_size': 512,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (devX.toarray(), devY.toarray()),\n",
    "    'shuffle': True\n",
    "}\n",
    "clf = KerasClassifier(**model_params)\n",
    "\n",
    "\n",
    "hist = clf.fit(trainX.toarray(), trainY.toarray())\n",
    "\n",
    "yhat = clf.predict(testX.toarray())\n",
    "\n",
    "# 0=b 1=i 2=o\n",
    "yhat = pd.DataFrame(yhat)\n",
    "yhat[yhat[0] == 0]  = 'b'\n",
    "yhat[yhat[0] == 1]  = 'i'\n",
    "yhat[yhat[0] == 2]  = 'o'\n",
    "\n",
    "\n",
    "d = {'y': test[\"label\"].values, 'yhat': yhat[0].values}\n",
    "comparison = pd.DataFrame(data=d)\n",
    "spanEval = pd.DataFrame( columns=[\"y\", \"yhat\"])\n",
    "for i in range(0, comparison.shape[0]):\n",
    "    if(comparison['y'][i] == 'b'):\n",
    "        j = i+1\n",
    "        ystr = \"b\"\n",
    "        yhatstr = comparison['yhat'][i]\n",
    "        while(comparison['y'][j] == \"i\"):\n",
    "            ystr = ystr + comparison['y'][j]\n",
    "            yhatstr = yhatstr + comparison['yhat'][j]\n",
    "            j = j + 1\n",
    "        toAppend = pd.DataFrame({'y':ystr, 'yhat':yhatstr}, index={1})\n",
    "        spanEval = pd.concat([spanEval, toAppend], ignore_index=True)\n",
    "\n",
    "spanEval[(spanEval['y'] == spanEval['yhat'])].shape[0] / spanEval.shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 1)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 1)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 2)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 2)].shape[0]\n",
    "spanEval[(spanEval['y'] == spanEval['yhat']) & (spanEval['yhat'].str.len() == 3)].shape[0] / spanEval[(spanEval['yhat'].str.len() == 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a71109-f31a-44f6-8d21-cebbef0be93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "20.8 18.9 20.8 21.1 17.7 17.7 21.1 18.32\n",
    "29.13 28.15 29.13 28.16 28.15 29.13 22.4 26.2\n",
    "25.3 28.16 23.9 26.8 18.84 1.67 33 18.84\n",
    "2.94 2.94 5.88 2.94 2.94 5.88 4.41 8.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Terminology]",
   "language": "python",
   "name": "conda-env-Terminology-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
