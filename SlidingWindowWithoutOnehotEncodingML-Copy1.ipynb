{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0447e63-fd42-4cf2-be03-8aa19b2b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, pipeline, svm, linear_model, neighbors, metrics, ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8561f3e9-b188-4166-8e0d-48859b58a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \".\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddf5fab5-0811-4eed-85f7-22735a3dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(search_path):\n",
    "    result = []\n",
    "    # Wlaking top-down from the root\n",
    "    for root, dir, files in os.walk(search_path):\n",
    "        for file in files:       \n",
    "                result.append(os.path.join(root, file))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca0db0a9-c5fa-487b-93b8-e9bf028b9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = find_files(dataPath + \"\\\\train\")\n",
    "testFiles = find_files(dataPath + \"\\\\test\")\n",
    "devFiles = find_files(dataPath + \"\\\\dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a5a3677-8b19-4e7a-9644-d170a856d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(paths):\n",
    "    data = pd.DataFrame(columns=[\"tokens\", \"label\"])\n",
    "    for i in paths:\n",
    "        try:\n",
    "            doc = pd.read_csv(i, sep=\"\\t\", names=[\"tokens\", \"label\"], header=None)\n",
    "            doc['file'] = i[12:-5]\n",
    "            data = pd.concat([data, doc], ignore_index=True)\n",
    "        except Exception as e: \n",
    "            print(i, e)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aac53450-2fc6-460f-8f4d-f499415126a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size before Cleaning\n",
      "Training Size:\t (21548, 3)\n",
      "Test Size:\t (2781, 3)\n",
      "Dev Size:\t (2414, 3)\n"
     ]
    }
   ],
   "source": [
    "train = loadData(trainFiles)\n",
    "test  = loadData(testFiles)\n",
    "dev = loadData(devFiles)\n",
    "print(\"Training Size before Cleaning\")\n",
    "print(\"Training Size:\\t\", train.shape)\n",
    "print(\"Test Size:\\t\", test.shape)\n",
    "print(\"Dev Size:\\t\", dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a22b72be-72c5-439b-b3dd-82894d6eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"about\", \"all\", \"also\", \"among\", \"at\", \"available\", \"be\", \"because\", \"been\", \"both\", \"but\", \"by\", \"can\", \"each\", \"first\", \"has\", \"have\", \"here\", \"how\",\n",
    "             \"however\", \"into\", \"it\", \"its\", \"large\", \"learn\", \"many\", \"may\", \"more\", \"most\", \"much\", \"new\", \"not\", \"often\", \"only\", \"or\", \"other\", \"over\", \"recent\", \"related\", \"same\",\n",
    "             \"several\", \"shown\", \"some\", \"studies\", \"such\", \"than\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"those\", \"through\", \"use\", \"used\", \"we\", \"well\", \"what\",\n",
    "             \"when\", \"where\", \"which\"]\n",
    "def stripSpaces(x):\n",
    "    x = unidecode(x)\n",
    "    specialchar = \"!@#$%^&*()[]{};:,./<>?\\|`-~=_+\\t\\n\"\n",
    "    for tag in specialchar:\n",
    "        x = x.replace(tag, '')\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def cleaning(_dataset, verbose=True):\n",
    "    dataset = _dataset.copy()\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isna()].index, inplace=True)\n",
    "    if verbose: \n",
    "        print(\"Size after Dropping Null Tokens\",dataset.shape)\n",
    "        print(\"Tokens Without labels:\")\n",
    "    for indexWithNullLabel in dataset[dataset[\"label\"].isna()].index:\n",
    "        token = dataset[\"tokens\"][indexWithNullLabel]\n",
    "        #split with ' ' doesnt consider multiple spaces as one\n",
    "        tokenslist = token.split()\n",
    "        dataset[\"tokens\"][indexWithNullLabel] = tokenslist[0]\n",
    "\n",
    "        if (len(tokenslist) > 1):\n",
    "            dataset[\"label\"][indexWithNullLabel] = tokenslist[1]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(dataset.loc[indexWithNullLabel, :])\n",
    "            #Manual Correction for 5467 and 5858 (very & research)\n",
    "            dataset[\"label\"][indexWithNullLabel] = 'O'\n",
    "            if verbose:\n",
    "                print(\"Manual Corrected:\", dataset[\"tokens\"][indexWithNullLabel])\n",
    "    dataset = dataset.applymap(stripSpaces)\n",
    "    #label to handel 0, i*, b*, o*, 0*\n",
    "    dataset[dataset[\"label\"] == 'ii'] = 'i'\n",
    "    dataset[dataset[\"label\"] == '0'] = 'o'\n",
    "    if verbose:\n",
    "        print(\"Removing special characters\")\n",
    "    specialCharTokens = dataset[~(dataset[\"tokens\"].str.isalnum())][\"tokens\"].unique()\n",
    "    #for sprecialChar with label B, moving label to next row and droping rows  \n",
    "    specialCharWithB = dataset[dataset[\"tokens\"].isin(specialCharTokens) & (dataset[\"label\"] == 'b')].index\n",
    "    for i in specialCharWithB:\n",
    "        dataset.loc[i+1, \"label\"] = 'b'\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(specialCharTokens) & ((dataset[\"label\"] == 'o') | (dataset[\"label\"] == 'b') )].index, inplace=True)\n",
    "    #Drop i where there is i and b before it\n",
    "    toDrop = []\n",
    "    for i in dataset[dataset[\"tokens\"].isin(specialCharTokens)].index:\n",
    "        if(dataset[\"label\"][i-1] == 'b' or dataset[\"label\"][i-1] == 'i' ):\n",
    "            toDrop.append(i)\n",
    "        else:\n",
    "            dataset[\"label\"][i] = 'b'\n",
    "    dataset.drop(toDrop, axis=0, inplace=True)\n",
    "    if verbose:\n",
    "        print(dataset.value_counts()[:30])\n",
    "        print(\"Removing Stopwords based on above listed most frequent words\")\n",
    "    stopwords = [\"the\",\"this\",\"that\",\"has\",\"have\",\"can\",\"be\",\"in\",\"on\",\"at\",\"to\",\"as\",\"is\",\"are\",\"a\",\"an\",\"with\",\"our\",\"we\",\"from\",\"which\",\"when\",\"also\",\"and\",\"or\",\"not\",\"it\",\"its\",\n",
    "                 \"than\",\"use\",\"into\",\"how\",\"but\",\"to\",\"for\",\"their\",\"there\",\"all\"]\n",
    "    if verbose:\n",
    "        print(\"Label order correction:\")\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    temp = dataset.copy()\n",
    "    temp[\"before\"] = temp[\"label\"].shift(1)\n",
    "    temp[\"after\"] = temp[\"label\"].shift(-1)\n",
    "    for i in temp[(temp[\"label\"] == 'i') & (temp[\"before\"] == 'o') ].index:\n",
    "            # oio or oii\n",
    "            if verbose:\n",
    "                print(temp.loc[i-1, \"tokens\"]+\"(\"+temp.loc[i-1, \"label\"]+\")\\t\\t\", temp.loc[i, \"tokens\"]+\"(\"+temp.loc[i, \"label\"]+\")\\t\\t\", temp.loc[i+1, \"tokens\"]+\"(\"+temp.loc[i+1, \"label\"]+\")\")\n",
    "            if(temp.loc[i+1, \"label\"] == 'o' or temp.loc[i+1, \"label\"] == 'i'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "            # oib\n",
    "            if(temp.loc[i+1, \"label\"] == 'b'):\n",
    "                dataset.loc[i, \"label\"] = 'b'\n",
    "                dataset.loc[i+1, \"label\"] = 'i'\n",
    "    del temp\n",
    "    if verbose:\n",
    "        print(dataset[(dataset[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index)\n",
    "    dataset.drop(dataset[dataset[\"tokens\"].isin(dataset)].index, inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3021768c-85f8-4499-8662-66140c10d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cleaning(train, verbose=False)\n",
    "test = cleaning(test, verbose=False)\n",
    "dev = cleaning(dev, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa3b82ad-bba3-4614-9941-7ba24bb44beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of and the is often used in between \n",
    "# And seprates the Keywords may be good to not remove index [:]\n",
    "# Arabic NER only NER is Keyword\n",
    "# languages, multiple languages, as keyword\n",
    "# linguistic phenomena across languages\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# for i in temp[(temp[\"tokens\"].isin(temp[(temp[\"tokens\"].isin(stopwords))][\"tokens\"].value_counts().index)) & (temp[\"label\"] != 'o')].index[:]:\n",
    "#     print(temp.loc[i, 'file'] +\"(\",i,\"):\"+\n",
    "#           temp.loc[i-4, 'tokens']+\"(\" + temp.loc[i-4, 'label'] +\") \"+ temp.loc[i-3, 'tokens']+\"(\" + temp.loc[i-3, 'label'] +\") \"+\n",
    "#           temp.loc[i-2, 'tokens']+\"(\" + temp.loc[i-2, 'label'] +\") \"+ temp.loc[i-1, 'tokens']+\"(\" + temp.loc[i-1, 'label'] +\") \"+\n",
    "#           temp.loc[i, 'tokens']+\"(\" + temp.loc[i, 'label'] +\") \"+ temp.loc[i+1, 'tokens']+\"(\" + temp.loc[i+1, 'label'] +\") \"+\n",
    "#           temp.loc[i+2, 'tokens']+\"(\" + temp.loc[i+2, 'label'] +\") \"+ temp.loc[i+3, 'tokens']+\"(\" + temp.loc[i+3, 'label'] +\") \")\n",
    "#\n",
    "#\n",
    "#\n",
    "#Removing these from stopwords as \n",
    "# there may be contained in a key word\n",
    "#and, of, on, all, for\n",
    "# leading tokens for keywords\n",
    "#the, a, and, for, in, to, on, that, as, with, an, our, from, this, is, are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1546c0b9-336c-4ba0-be4c-650184bea439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurePreparation(_dataset):\n",
    "    dataset = _dataset.copy()\n",
    "    dataset[\"t1\"] = dataset[\"tokens\"].shift(3)\n",
    "    dataset[\"t2\"] = dataset[\"tokens\"].shift(2)\n",
    "    dataset[\"t3\"] = dataset[\"tokens\"].shift()\n",
    "    dataset[\"t4\"] = dataset[\"tokens\"] #term to classify\n",
    "    dataset[\"t5\"] = dataset[\"tokens\"].shift(-1)\n",
    "    dataset[\"t6\"] = dataset[\"tokens\"].shift(-2)\n",
    "    dataset[\"t7\"] = dataset[\"tokens\"].shift(-3)\n",
    "    dataset[\"file2\"] = dataset[\"file\"].shift()\n",
    "    dataset.loc[dataset[dataset[\"file\"] != dataset[\"file2\"]].index, [\"t1\", \"t2\", \"t3\"]] = [float('nan'), float('nan'), float('nan')]\n",
    "    dataset = dataset.drop([\"tokens\", \"file2\", \"file\"], axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4b26e3c-8763-4007-9176-4d0948a9cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = featurePreparation(train)\n",
    "test = featurePreparation(test)\n",
    "dev = featurePreparation(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46a73713-3048-4343-9dae-cfaa83ddaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #investigating small tokens\n",
    "# print(\"len(Tokens) <= 1 and is term\")\n",
    "# train[(train[\"t4\"].str.len() == 1) & (train[\"label\"] != 'o')][\"t4\"].unique()\n",
    "# print(\"len(Tokens) = 1\")\n",
    "# train[(train[\"t4\"].str.len() == 1)][\"t4\"].unique()\n",
    "# print(\"len(Tokens) = 2\")\n",
    "# train[(train[\"t4\"].str.len() == 2) & (train[\"label\"] != 'o')][\"t4\"].unique()\n",
    "# print(\"len(Tokens) = 2 and is term\")\n",
    "# train[(train[\"t4\"].str.len() == 2)][\"t4\"].unique()\n",
    "# print(\"len(Tokens) = 1\")\n",
    "# train[(train[\"t4\"].str.len() == 3) & (train[\"label\"] != 'o')][\"t4\"].unique()\n",
    "# print(\"len(Tokens) = 1 and is term\")\n",
    "# train[(train[\"t4\"].str.len() == 3)][\"t4\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfecd93e-77e8-4efa-8597-96758464f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = train.t4.unique()\n",
    "tokens = np.append(tokens, '_nan')\n",
    "tokens = np.append(tokens, 'new_token')\n",
    "decoderDic = dict(enumerate(tokens, 1))\n",
    "encoderDic = {v: k for k, v in decoderDic.items()}\n",
    "labelencodeing = {'b': 0, 'i': 1, 'o': 2}\n",
    "labeldecodeing = {v: k for k, v in labelencodeing.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07e74e20-7830-442b-8a0f-86bdd54d4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(token):\n",
    "    try:\n",
    "        return encoderDic[token]\n",
    "    except:\n",
    "        if(token == float('nan')):\n",
    "            return encoderDic[\"_nan\"]\n",
    "        else:\n",
    "            return encoderDic[\"new_token\"]\n",
    "def decode(embedding):\n",
    "    try:\n",
    "        return decoderDic[embedding]\n",
    "    except:\n",
    "        if(embedding == encoderDic[\"_nan\"]):\n",
    "            return decoderDic[\"_nan\"]\n",
    "        elif(embedding == encoderDic[\"new_token\"]):\n",
    "            return decoderDic[\"new_token\"]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "def encodeDataset(_dataset, train=False):\n",
    "    dataset = _dataset.copy()\n",
    "    dataset[\"t1\"] = dataset[\"t1\"].map(encode)\n",
    "    dataset[\"t2\"] = dataset[\"t2\"].map(encode) \n",
    "    dataset[\"t3\"] = dataset[\"t3\"].map(encode) \n",
    "    dataset[\"t4\"] = dataset[\"t4\"].map(encode) \n",
    "    dataset[\"t5\"] = dataset[\"t5\"].map(encode) \n",
    "    dataset[\"t6\"] = dataset[\"t6\"].map(encode) \n",
    "    dataset[\"t7\"] = dataset[\"t7\"].map(encode)\n",
    "    dataset[\"label\"] = dataset[\"label\"].map(lambda x: labelencodeing[x] )\n",
    "    return dataset[\"label\"], dataset[[\"t1\",\"t2\",\"t3\",\"t4\",\"t5\",\"t6\",\"t7\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9d3384e-bbcc-4e1d-acd3-b79b0fdd671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY, trainX = encodeDataset(train)\n",
    "devY, devX = encodeDataset(dev)\n",
    "testY, testX = encodeDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8c07f07-c28b-4a3d-81fd-78c19fc6857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline.make_pipeline(preprocessing.StandardScaler(),  ensemble.RandomForestClassifier())\n",
    "pipe.fit(trainX, trainY)\n",
    "y_hat = pipe.predict(testX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b73e337-c50c-4192-9ec0-eef0f79ae37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.49      0.22      0.30       322\n",
      "           I       0.54      0.24      0.33       315\n",
      "           O       0.79      0.95      0.86      1823\n",
      "\n",
      "    accuracy                           0.76      2460\n",
      "   macro avg       0.61      0.47      0.50      2460\n",
      "weighted avg       0.72      0.76      0.72      2460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(testY, y_hat, target_names=['B', 'I', 'O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7105eb0a-ba47-4ee9-932f-4205c329d385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  62,   28,  232],\n",
       "       [  20,   75,  220],\n",
       "       [  55,   41, 1727]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(testY, y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDMC",
   "language": "python",
   "name": "idmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
